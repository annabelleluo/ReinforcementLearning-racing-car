{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f403d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=Warning)\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from gym.envs.box2d import CarRacing\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines.deepq.policies import CnnPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv,VecVideoRecorder\n",
    "from stable_baselines import DQN\n",
    "import glob\n",
    "import base64\n",
    "import wandb\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import Any, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83d314",
   "metadata": {},
   "source": [
    "## General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "603fceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "\n",
    "class DQNCustomCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super(DQNCustomCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseAlgorithm\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # stable_baselines3.common.logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.episodes = 0\n",
    "        self.total_episode_reward = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.episodes ==0:\n",
    "            self.episodes += 1\n",
    "        else:\n",
    "            self.total_episode_reward += self.locals[\"reward_\"]\n",
    "            # at the end of every episode       \n",
    "            if self.locals[\"done\"]:\n",
    "                if self.episodes % self.locals[\"log_interval\"] != 0: \n",
    "                    wandb.log({\"reward_per_episode\": self.total_episode_reward})\n",
    "\n",
    "                # if log interval has passed\n",
    "                if self.episodes % self.locals[\"log_interval\"] == 0:\n",
    "                    # Save your model and optimizer\n",
    "                    self.model.save(MODEL_SAVE_NAME)\n",
    "                    # Save as artifact for version control.\n",
    "                    artifact = wandb.Artifact(MODEL_SAVE_NAME, type='model')\n",
    "                    artifact.add_file(MODEL_SAVE_NAME+\".zip\")\n",
    "                    wandb.log_artifact(artifact)\n",
    "                    wandb.log({\"reward_per_episode\": self.total_episode_reward})\n",
    "                self.episodes += 1\n",
    "                self.total_episode_reward = 0\n",
    "\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the learn() method.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27622387",
   "metadata": {},
   "source": [
    "# Baseline environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659cb250",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cc122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = lambda : CarRacing(\n",
    "        grayscale=1,\n",
    "        show_info_panel=0,\n",
    "        discretize_actions=\"hard\",\n",
    "        frames_per_state=4,\n",
    "        num_lanes=1,\n",
    "        num_lanes_changes=1,\n",
    "        num_tracks=1,\n",
    "        allow_reverse=False,\n",
    "        max_time_out=2,\n",
    "        verbose=0,\n",
    "        num_obstacles=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71d7c27b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mant_ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\antoi\\Downloads\\Project RL\\wandb\\run-20230413_234734-3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ant_ai/DQN_No_Obstacles/runs/3' target=\"_blank\">3</a></strong> to <a href='https://wandb.ai/ant_ai/DQN_No_Obstacles' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ant_ai/DQN_No_Obstacles' target=\"_blank\">https://wandb.ai/ant_ai/DQN_No_Obstacles</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ant_ai/DQN_No_Obstacles/runs/3' target=\"_blank\">https://wandb.ai/ant_ai/DQN_No_Obstacles/runs/3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INITIALIZE NEW DQN MODEL\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 75       |\n",
      "| episodes                | 50       |\n",
      "| mean 100 episode reward | -141     |\n",
      "| steps                   | 24777    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 44       |\n",
      "| episodes                | 100      |\n",
      "| mean 100 episode reward | -146     |\n",
      "| steps                   | 56405    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 26       |\n",
      "| episodes                | 150      |\n",
      "| mean 100 episode reward | -137     |\n",
      "| steps                   | 75208    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 13       |\n",
      "| episodes                | 200      |\n",
      "| mean 100 episode reward | -119     |\n",
      "| steps                   | 88372    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 250      |\n",
      "| mean 100 episode reward | -112     |\n",
      "| steps                   | 101504   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 300      |\n",
      "| mean 100 episode reward | -109     |\n",
      "| steps                   | 111691   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 350      |\n",
      "| mean 100 episode reward | -109     |\n",
      "| steps                   | 122227   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 400      |\n",
      "| mean 100 episode reward | -109     |\n",
      "| steps                   | 132397   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 450      |\n",
      "| mean 100 episode reward | -107     |\n",
      "| steps                   | 142970   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 500      |\n",
      "| mean 100 episode reward | -107     |\n",
      "| steps                   | 153001   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 550      |\n",
      "| mean 100 episode reward | -107     |\n",
      "| steps                   | 164478   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 600      |\n",
      "| mean 100 episode reward | -108     |\n",
      "| steps                   | 175342   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 650      |\n",
      "| mean 100 episode reward | -105     |\n",
      "| steps                   | 184236   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 700      |\n",
      "| mean 100 episode reward | -105     |\n",
      "| steps                   | 195268   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 750      |\n",
      "| mean 100 episode reward | -107     |\n",
      "| steps                   | 205612   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| % time spent exploring  | 2        |\n",
      "| episodes                | 800      |\n",
      "| mean 100 episode reward | -106     |\n",
      "| steps                   | 214291   |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_44560\\1157332484.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[0mDQNmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLOG_INTERVAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDQNCustomCallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[0mDQNmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_SAVE_NAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\stable_baselines\\deepq\\dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[0;32m    294\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m                         _, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1, dones, weights,\n\u001b[1;32m--> 296\u001b[1;33m                                                         sess=self.sess)\n\u001b[0m\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprioritized_replay\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\stable_baselines\\common\\tf_util.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sess, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs_update\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\project_rl\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num = 3\n",
    "\n",
    "LOG_INTERVAL        = 50\n",
    "WANDB_ID            = \"\" + str(num)\n",
    "WNDB_NAME           = \"Antoine\" + str(num)\n",
    "LOAD_SAVED_MODEL    = False\n",
    "MODEL_SAVE_NAME     = \"DQN_MODEL_\" + str(num)\n",
    "SAVED_MODEL_VERSION = \"latest\"\n",
    "BUFFER_SIZE         = 150000\n",
    "LEARNING_STARTS     = 3000\n",
    "\n",
    "os.environ[\"WANDB_ENTITY\"]  = \"ant_ai\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"DQN_No_Obstacles\"\n",
    "os.environ[\"WANDB_RESUME\"]  = \"allow\"\n",
    "wandb.init(resume=WANDB_ID)\n",
    "wandb.run.name = WNDB_NAME\n",
    "\n",
    "env = make_vec_env(env, n_envs=1)\n",
    "\n",
    "# Load model\n",
    "if LOAD_SAVED_MODEL:\n",
    "    try:\n",
    "        model_artifact = wandb.use_artifact(MODEL_SAVE_NAME+':'+SAVED_MODEL_VERSION, type='model')\n",
    "        artifact_dir = model_artifact.download()\n",
    "        DQNmodel = DQN.load(artifact_dir+\"/\"+MODEL_SAVE_NAME, env=env)\n",
    "        print(\"LOAD SAVED DQN MODEL\")\n",
    "\n",
    "    except:\n",
    "        print(\"NO MODEL FOUND\")\n",
    "else:\n",
    "    if 'DQNmodel' not in globals():\n",
    "        DQNmodel = DQN(CnnPolicy, env, verbose=1, buffer_size=BUFFER_SIZE, learning_starts=LEARNING_STARTS)\n",
    "        print(\"INITIALIZE NEW DQN MODEL\")\n",
    "    else:\n",
    "        DQNmodel = DQN.load(MODEL_SAVE_NAME, env=env)\n",
    "        print(\"CONTINUE DQN MODEL TRAINING\")\n",
    "\n",
    "        # Train model\n",
    "DQNmodel.learn(total_timesteps=1000000, log_interval=LOG_INTERVAL, callback=DQNCustomCallback())\n",
    "DQNmodel.save(MODEL_SAVE_NAME)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "402d0e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>reward_per_episode</td><td>▄▆▁▂▃▅▅▅▄▅▅▅▆▆▆▆▆▆▆▅▅▇▆▆▆█▆▆▇▆▆▆▅▆▆▇▄▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>reward_per_episode</td><td>-105.6</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">3</strong> at: <a href='https://wandb.ai/ant_ai/DQN_No_Obstacles/runs/3' target=\"_blank\">https://wandb.ai/ant_ai/DQN_No_Obstacles/runs/3</a><br/>Synced 6 W&B file(s), 0 media file(s), 16 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230413_234734-3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23be86d3",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d931c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = lambda : CarRacing(\n",
    "        grayscale=1,\n",
    "        show_info_panel=0,\n",
    "        discretize_actions=\"hard\",\n",
    "        frames_per_state=4,\n",
    "        num_lanes=1,\n",
    "        num_lanes_changes=1,\n",
    "        num_tracks=1,\n",
    "        allow_reverse=False,\n",
    "        max_time_out=2,\n",
    "        verbose=0)\n",
    "env = DummyVecEnv([env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6efb5a7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: [-44.699608]\n",
      "Episode: 1, Total Reward: [-105.7]\n",
      "Episode: 2, Total Reward: [111.40096]\n",
      "Episode: 3, Total Reward: [-105.7]\n",
      "Episode: 4, Total Reward: [-105.7]\n",
      "Episode: 5, Total Reward: [-58.999573]\n",
      "Episode: 6, Total Reward: [-104.7]\n",
      "Episode: 7, Total Reward: [-50.499493]\n",
      "Episode: 8, Total Reward: [-96.60005]\n",
      "Episode: 9, Total Reward: [-48.598885]\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load('Model Weights/DQN/Baseline Environment Agent/DQN_MODEL_1-v50/DQN_MODEL_1',env=env)\n",
    "\n",
    "model.set_env(env)\n",
    "\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done=False\n",
    "    while not done:\n",
    "        action, states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "    print(f'Episode: {episode}, Total Reward: {total_reward}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c7fb6",
   "metadata": {},
   "source": [
    "## Record Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4e057a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Complexity is set through 3 parameters: \n",
    "num_tracks: 1 for basic track, 2 for more complex tracks with intersection (X, t type)\n",
    "num_lanes: number of lanes for the track 1 for normal, 2 for multiple lanes\n",
    "num_lanes_changes: number of time two lanes will be merged into 1 over the track\n",
    "\n",
    "\n",
    "Obstacles/Bonus are controled by 2 parameters\n",
    "num_obstacles: number of obstacles by sub section of the track (if different than 0 then there are obstacles)\n",
    "If the car touched an obstacle, 50 points are deducted from the score\n",
    "prop_good_obstacles: probability (set between 0-1) for an obstacles to be a bonus (yellow color instead of red + reward 50 points if touched)\n",
    "'''\n",
    "\n",
    "\n",
    "env = lambda : CarRacing(\n",
    "        grayscale=1,\n",
    "        show_info_panel=0,\n",
    "        discretize_actions=\"hard\",\n",
    "        frames_per_state=4,\n",
    "        num_lanes=1,\n",
    "        num_lanes_changes=1,\n",
    "        num_tracks=1,\n",
    "        allow_reverse=False,\n",
    "        max_time_out=2,\n",
    "        verbose=0,\n",
    "        num_obstacles=0)\n",
    "env = DummyVecEnv([env])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb4adf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: [-105.7]\n",
      "Episode: 1, Total Reward: [-86.70007]\n",
      "Episode: 2, Total Reward: [-57.998413]\n",
      "Episode: 3, Total Reward: [-106.09999]\n",
      "Episode: 4, Total Reward: [-32.099487]\n",
      "Episode: 5, Total Reward: [-105.7]\n",
      "Episode: 6, Total Reward: [-105.7]\n",
      "Episode: 7, Total Reward: [-107.2]\n",
      "Episode: 8, Total Reward: [-105.29999]\n",
      "Episode: 9, Total Reward: [-104.7]\n"
     ]
    }
   ],
   "source": [
    "model = DQN.load('Model Weights/DQN/Baseline Environment Agent/DQN_MODEL_1-v50/DQN_MODEL_1',env=env) # Baseline agent trained on the base environment\n",
    "\n",
    "# Record the video starting at the first step\n",
    "env = VecVideoRecorder(env, 'video/',\n",
    "                       record_video_trigger=lambda x: x == 0, video_length=1500,\n",
    "                       name_prefix=\"DQN_agent-based\")\n",
    "\n",
    "for episode in range(10):\n",
    "    obs = env.reset()\n",
    "    total_reward = 0\n",
    "    done=False\n",
    "    while not done:\n",
    "        action, states = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        env.render()\n",
    "    print(f'Episode: {episode}, Total Reward: {total_reward}')\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_rl",
   "language": "python",
   "name": "project_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
