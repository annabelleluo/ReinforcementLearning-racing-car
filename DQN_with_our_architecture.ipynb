{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b2a81",
   "metadata": {
    "id": "334b2a81"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from gym.envs.box2d import CarRacing\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, BatchNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TwOwWgo9VkAb",
   "metadata": {
    "id": "TwOwWgo9VkAb"
   },
   "source": [
    "# Baseline environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XAlN358NY83q",
   "metadata": {
    "id": "XAlN358NY83q"
   },
   "source": [
    "## Build our own DQN archtechture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41083f08",
   "metadata": {
    "id": "41083f08"
   },
   "outputs": [],
   "source": [
    "class CarRacingDQNAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space    = 5,\n",
    "        frame_stack_num = 4,\n",
    "        memory_size     = 5000,\n",
    "        gamma           = 0.99,  # discount rate\n",
    "        epsilon         = 1.0,   # exploration rate\n",
    "        epsilon_min     = 0.1,\n",
    "        epsilon_decay   = 0.9999,\n",
    "        learning_rate   = 0.001,\n",
    "        SAVE_FREQUENCY = 100\n",
    "    ):\n",
    "        self.action_space    = action_space\n",
    "        self.frame_stack_num = frame_stack_num\n",
    "        self.memory          = deque(maxlen=memory_size)\n",
    "        self.gamma           = gamma\n",
    "        self.epsilon         = epsilon\n",
    "        self.epsilon_min     = epsilon_min\n",
    "        self.epsilon_decay   = epsilon_decay\n",
    "        self.learning_rate   = learning_rate\n",
    "        self.SAVE_FREQUENCY  = SAVE_FREQUENCY\n",
    "        self.model           = self.build_model()\n",
    "        self.target_model    = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "\n",
    "# The First archtecture \n",
    "\n",
    "    #def build_model(self):\n",
    "        #Neural Net for Deep-Q learning Model\n",
    "        #model = Sequential()\n",
    "        #model.add(Conv2D(filters=6, kernel_size=(7, 7), strides=3, activation='relu', input_shape=(96, 96, self.frame_stack_num)))\n",
    "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(filters=12, kernel_size=(4, 4), activation='relu'))\n",
    "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(216, activation='relu'))\n",
    "        #model.add(Dense(self.action_space, activation='softmax'))\n",
    "        #model.compile(loss='mean_squared_error', optimizer=Adam(lr=self.learning_rate, epsilon=1e-7))\n",
    "        #return model\n",
    "\n",
    "\n",
    "# The Second architecture \n",
    "    #def build_model(self):\n",
    "        #Neural Net for Deep-Q learning Model\n",
    "        #model = Sequential()\n",
    "        #model.add(Conv2D(filters=6, kernel_size=(7, 7), strides=3, activation='relu', input_shape=(96, 96, self.frame_stack_num)))\n",
    "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Conv2D(filters=12, kernel_size=(4, 4), activation='relu'))\n",
    "        #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(216, activation='relu'))\n",
    "        #model.add(Dense(128, activation='relu'))  # added new dense layer\n",
    "        #model.add(Dense(64, activation='relu'))   # added another dense layer\n",
    "        #model.add(Dense(self.action_space, activation='softmax'))\n",
    "        #model.compile(loss='mean_squared_error', optimizer=Adam(lr=self.learning_rate, epsilon=1e-7))\n",
    "       #return model\n",
    "\n",
    "# The Third archtecture\n",
    "    #def build_model(self):\n",
    "    # Neural Net for Deep-Q learning Model\n",
    "        #model = Sequential()\n",
    "        #model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=( 96, 96, 4), data_format='channels_first'))\n",
    "        #model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu', data_format='channels_first'))\n",
    "        #model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu', data_format='channels_first'))\n",
    "        #model.add(Flatten())\n",
    "        #model.add(Dense(512, activation='relu'))\n",
    "        #model.add(Dense(self.action_space, activation='linear'))\n",
    "        #model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate, epsilon=1e-7))\n",
    "        #return model\n",
    "\n",
    "\n",
    "# The Fourth archtechture\n",
    "    def build_model(self):\n",
    "    # Neural Net for Deep-Q learning Model\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=64, kernel_size=(8, 8), strides=4, activation='relu', input_shape=(4, 96, 96), padding='same'))\n",
    "        model.add(Conv2D(filters=128, kernel_size=(4, 4), strides=2, activation='relu', padding='same'))\n",
    "        model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=256, kernel_size=(3, 3), strides=1, activation='relu', padding='same'))\n",
    "        model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=1, activation='relu', padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=512, kernel_size=(3, 3), strides=1, activation='relu', padding='same'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_space, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate, epsilon=1e-7))\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            act_values = self.model.predict(np.expand_dims(state, axis=0))\n",
    "            action_index = np.argmax(act_values[0])\n",
    "        else:\n",
    "            action_index = random.randint(0,self.action_space-1)\n",
    "        return action_index\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        train_state = []\n",
    "        train_target = []\n",
    "        for state, action_index, reward, next_state, done in minibatch:\n",
    "            state = np.array(state) #convert deque to numpy array\n",
    "            #target = self.model.predict(np.expand_dims(state, axis=0))[0]\n",
    "            target = self.model.predict(state.reshape(1, *state.shape))[0]  # calculate Q-values\n",
    "            if done:\n",
    "                target[action_index] = reward\n",
    "            else:\n",
    "                t = self.target_model.predict(np.expand_dims(next_state, axis=0))[0]\n",
    "                target[action_index] = reward + self.gamma * np.amax(t)\n",
    "            train_state.append(state)\n",
    "            train_target.append(target)\n",
    "        self.model.fit(np.array(train_state), np.array(train_target), epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.update_target_model()\n",
    "\n",
    "    def save(self, name):\n",
    "        self.target_model.save_weights(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e462ffe",
   "metadata": {
    "id": "0e462ffe"
   },
   "outputs": [],
   "source": [
    "def process_state_image(state):\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_BGR2GRAY)\n",
    "    state = state.astype(float)\n",
    "    state /= 255.0\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9879901a",
   "metadata": {
    "id": "9879901a"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb2d5e",
   "metadata": {
    "id": "32eb2d5e"
   },
   "outputs": [],
   "source": [
    "RENDER                        = True\n",
    "STARTING_EPISODE              = 1\n",
    "ENDING_EPISODE                = 2000\n",
    "SKIP_FRAMES                   = 2\n",
    "TRAINING_BATCH_SIZE           = 64\n",
    "SAVE_TRAINING_FREQUENCY       = 5\n",
    "UPDATE_TARGET_MODEL_FREQUENCY = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2e0fc7",
   "metadata": {
    "id": "1e2e0fc7",
    "outputId": "d6874c6f-0c64-4706-a01d-1dfb13f594b7",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################\n",
      "Warning: making frames_per_state = 1\n",
      "No support for several frames in RGB\n",
      "WARNING:tensorflow:From C:\\Anaconda\\envs\\project_rl\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Episode: 1/2000, Scores(Time Frames): 49, Total Rewards(adjusted): -1.1e+02, Epsilon: 1.0\n",
      "Episode: 2/2000, Scores(Time Frames): 276, Total Rewards(adjusted): -1.6e+02, Epsilon: 0.97\n",
      "Episode: 3/2000, Scores(Time Frames): 88, Total Rewards(adjusted): -1.2e+02, Epsilon: 0.97\n",
      "Episode: 4/2000, Scores(Time Frames): 190, Total Rewards(adjusted): -1.5e+02, Epsilon: 0.95\n",
      "Episode: 5/2000, Scores(Time Frames): 164, Total Rewards(adjusted): -1.4e+02, Epsilon: 0.93\n",
      "Episode: 6/2000, Scores(Time Frames): 78, Total Rewards(adjusted): -1.2e+02, Epsilon: 0.93\n"
     ]
    }
   ],
   "source": [
    "env=CarRacing(\n",
    "        grayscale=0,\n",
    "        show_info_panel=0,\n",
    "        discretize_actions='hard',\n",
    "        frames_per_state=1,\n",
    "        num_lanes=1,\n",
    "        num_lanes_changes=1,\n",
    "        num_tracks=1,\n",
    "        allow_reverse=False,\n",
    "        max_time_out=2,\n",
    "        verbose=0\n",
    "        )\n",
    "\n",
    "agent = CarRacingDQNAgent()\n",
    "\n",
    "\n",
    "for e in range(STARTING_EPISODE, ENDING_EPISODE+1):\n",
    "    init_state = env.reset()\n",
    "    init_state=process_state_image(init_state)\n",
    "    total_reward = 0\n",
    "    negative_reward_counter = 0\n",
    "    state_frame_stack_queue = deque([init_state]*agent.frame_stack_num, maxlen=agent.frame_stack_num)\n",
    "    time_frame_counter = 1\n",
    "    done = False\n",
    "\n",
    "    while True:\n",
    "        if RENDER:\n",
    "            env.render('human')\n",
    "\n",
    "        current_state_frame_stack = state_frame_stack_queue #generate_state_frame_stack_from_queue(state_frame_stack_queue)\n",
    "        action = agent.act(current_state_frame_stack)\n",
    "\n",
    "        reward = 0\n",
    "        for _ in range(SKIP_FRAMES+1):\n",
    "            next_state, r, done, info = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        total_reward += reward\n",
    "\n",
    "        next_state = process_state_image(next_state)\n",
    "        state_frame_stack_queue.append(next_state)\n",
    "        next_state_frame_stack = state_frame_stack_queue #generate_state_frame_stack_from_queue(state_frame_stack_queue)\n",
    "\n",
    "        agent.memorize(current_state_frame_stack, action, reward, next_state_frame_stack, done)\n",
    "\n",
    "        if done:\n",
    "            print('Episode: {}/{}, Scores(Time Frames): {}, Total Rewards(adjusted): {:.2}, Epsilon: {:.2}'.format(e, ENDING_EPISODE, time_frame_counter, float(total_reward), float(agent.epsilon)))\n",
    "            break\n",
    "        if len(agent.memory) > TRAINING_BATCH_SIZE:\n",
    "            agent.replay(TRAINING_BATCH_SIZE)\n",
    "        time_frame_counter += 1\n",
    "\n",
    "    if e % UPDATE_TARGET_MODEL_FREQUENCY == 0:\n",
    "        agent.update_target_model()\n",
    "        \n",
    "    if e % SAVE_TRAINING_FREQUENCY == 0:    \n",
    "        save_path = os.path.join(\"C:\\DSBA\\Term 2\\Reinforcement Learning\\Project\\DQNmodel\", f\"car_racing_dqn_agent_{e}.h5\")\n",
    "        agent.save(\"C:\\DSBA\\Term 2\\Reinforcement Learning\\Project\\DQNmodel\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "project_rl",
   "language": "python",
   "name": "project_rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
